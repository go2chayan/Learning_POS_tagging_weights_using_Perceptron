README
======

I did it as a part of homework problem in the Statistical Speech and Language Processing course taught by 
Prof Daniel Gildea (https://www.cs.rochester.edu/~gildea/) in Fall 2015. Here I implemented a Perceptron
based training algorithm for Parts of Speech (POS) tagging problem. The training algorithm is shown in
algorithm.png file. Given a big dataset, the algorithm first randomly initializes the weights of a Hidden
Markov Model (HMM) and then uses Viterbi to predict the correct POS tag. When the tag is wrong, it updates
the weights according to Perceptron update rule.
==========================================================================================================

Name: Md. Iftekhar Tanveer
Email: itanveer@cs.rochester.edu  or  mtanveer@z.rochester.edu
Course: CS448
Homework 1: Implement perceptron training using data given in /u/cs448/data/pos.
What is your accuracy on the test file when training on the train file?


************** Files ***************
README: This document
alltags: List of all the tags.
hw2.py: The original python script. main function included. doesn't take any argument
outpput_plot.png: Output plot (discussed in Note)
weights (generated by the program) ... learned wieghts
train:............... Data files (Not included but assumed to exist in the same folder)
dev:  ............... (as above)
test: ............... (as above)
************* Algorithm ************
I've followed the algorithm
discussed in the class notes. The
code is well-commented. Please check
out the code for discussion on the algorithm.

************* Results **************
Total Words = 56684
Correctly tagged = 51603
accuracy= 91.0362712582%

************* Note *****************
For termination of the training process, I know that
we are supposed to train on the training data and check
the accuracy on the dev set. We are supposed to continue
training until the accuracy on dev set saturates. But
due to time constraints, I ran until a constant number
of iteration (5). However, I've calculated the accuracy
for both training and test dataset and plotted as shown
in outpput_plot.png. It is evident from the plot that
although the accuracy on dev set was not totally saturated,
but it is very close to saturation. 

I've saved the learnt weights in the "weights" file. There
are both positive and negative weights. As the perceptron
algorithm doesn't calculate the log probabilities, it is
totally meaningful to have +ve as well as -ve weights.
